import logging
from ..config import config
from .embedding_service import EmbeddingService
from .vector_store import VectorStore
from openai import OpenAI
from typing import List, Dict, Any

logger = logging.getLogger("nexusai.rag")

class RAGService:
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.vector_store = VectorStore()

        if config.LLM_PROVIDER == "ollama":
            self.llm_client = OpenAI(
                api_key="ollama",  # Ollama doesn't need a real key
                base_url=config.OLLAMA_BASE_URL,
            )
            logger.info(f"ğŸ¦™ LLM provider: Ollama ({config.OLLAMA_BASE_URL}) â€” model: {config.LLM_MODEL}")
        else:
            self.llm_client = OpenAI(
                api_key=config.DEEPSEEK_API_KEY,
                base_url=config.DEEPSEEK_BASE_URL,
            )
            logger.info(f"ğŸ¤– LLM provider: DeepSeek â€” model: {config.LLM_MODEL}")

    @staticmethod
    def _progress_bar() -> str:
        return "[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%"

    def generate_response(self, query: str, history: List[Dict[str, Any]] = None):
        if history is None:
            history = []
        
        # Step 1: Embed query
        logger.info(f"[1/4] ğŸ” å‘é‡åŒ–æŸ¥è¯¢ä¸­...")
        query_vector = self.embedding_service.get_embeddings([query])[0]
        logger.info(f"      {self._progress_bar()}  æŸ¥è¯¢å‘é‡åŒ–å®Œæˆ")

        # Step 2: Search relevant chunks
        logger.info(f"[2/4] ğŸ“š æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
        hits = self.vector_store.search(query_vector, limit=5)
        logger.info(f"      {self._progress_bar()}  æ£€ç´¢åˆ° {len(hits)} æ¡ç›¸å…³ç‰‡æ®µ")

        # Step 3: Build context and track sources
        logger.info(f"[3/4] ğŸ“ æ„å»ºæç¤ºè¯...")
        context_parts = []
        source_details = []
        
        for hit in hits:
            payload = hit["payload"]
            score = hit["score"]
            context_parts.append(payload["chunk_text"])
            source_details.append({
                "source_file": payload["source_file"],
                "content": payload["chunk_text"],
                "score": float(score)
            })

        context_text = "\n\n---\n\n".join(context_parts)
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹æ–¹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
            "å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦‚å®è¯´æ˜ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚å›ç­”è¯·ç®€æ´å‡†ç¡®ã€‚\n\n"
            f"å‚è€ƒæ–‡æ¡£ï¼š\n{context_text}"
        )
        
        log_sources = list(set([s["source_file"] for s in source_details]))
        logger.info(f"      {self._progress_bar()}  æ¥æºæ–‡ä»¶: {log_sources}")

        # Step 4: Call LLM (streaming)
        messages = [{"role": "system", "content": system_prompt}]
        
        # Embed chat history
        for msg in history:
            role = "assistant" if msg.get("isAi") else "user"
            messages.append({"role": role, "content": msg.get("text", "")})
            
        messages.append({"role": "user", "content": query})

        logger.info(f"[4/4] ğŸ¤– è°ƒç”¨ LLM ç”Ÿæˆå›ç­” (model={config.LLM_MODEL})...")
        response = self.llm_client.chat.completions.create(
            model=config.LLM_MODEL,
            messages=messages,
            stream=True
        )
        logger.info(f"      {self._progress_bar()}  LLM å¼€å§‹æµå¼è¾“å‡º...")

        return response, source_details
